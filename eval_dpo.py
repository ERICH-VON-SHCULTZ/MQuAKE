import torch
import json
import argparse
import re
import sys
import os
from typing import List, Dict, Any
from tqdm import tqdm
from datasets import load_dataset
from peft import PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)

# --- é»˜è®¤é…ç½® ---
DEFAULT_BASE_MODEL = "Qwen/Qwen3-8B"
# è¿™é‡ŒæŒ‡å‘ DPO è®­ç»ƒåçš„æœ€ç»ˆæ¨¡å‹è·¯å¾„
DEFAULT_LORA_PATH = "dpo/qwen3-8b-mquake-dpo-final" 
DEFAULT_DATASET_PATH = "datasets/MQuAKE-T.json"

# Special tokens
THINK_START_TOKEN = "<think>"
THINK_END_TOKEN = "</think>"

# --- è¾…åŠ©å‡½æ•° ---

def normalize_answer(s):
    """
    æ ‡å‡†åŒ–ç­”æ¡ˆï¼šè½¬å°å†™ï¼Œå»æ ‡ç‚¹ï¼Œå»å¤šä½™ç©ºæ ¼
    """
    s = str(s).lower().strip()
    s = re.sub(r'[^\w\s]', '', s)
    return " ".join(s.split())

def check_answer(generated_answer, expected_answer, aliases):
    """
    æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼ˆåŒ…å«å…³ç³»ï¼‰
    """
    gen_norm = normalize_answer(generated_answer)
    exp_norm = normalize_answer(expected_answer)
    
    # 1. ç›´æ¥åŒ…å«æ£€æŸ¥
    if exp_norm in gen_norm:
        return True
        
    # 2. åˆ«ååŒ…å«æ£€æŸ¥
    if aliases:
        for alias in aliases:
            alias_norm = normalize_answer(alias)
            if alias_norm and alias_norm in gen_norm:
                return True
    
    return False

def get_batch_responses(model, tokenizer, prompts: List[str], enable_thinking=False, batch_size=32, max_new_tokens=1024) -> List[Dict[str, str]]:
    """
    æ‰¹é‡ç”Ÿæˆå‡½æ•°
    """
    all_responses = []
    
    for i in tqdm(range(0, len(prompts), batch_size), desc=f"æ¨ç†è¿›åº¦ (think={enable_thinking})"):
        batch_prompts = prompts[i:i+batch_size]
        
        # 1. å‡†å¤‡ Chat æ¨¡æ¿
        # æ³¨æ„ï¼šQwen3 çš„ chat template ä¸éœ€è¦ç³»ç»Ÿæç¤ºè¯ï¼Œä¸”æ”¯æŒ system role
        batch_messages = [[{"role": "user", "content": p}] for p in batch_prompts]
        
        try:
            texts = tokenizer.apply_chat_template(
                batch_messages,
                tokenize=False,
                add_generation_prompt=True
                # Qwen3-8B é€šå¸¸ä¸éœ€è¦ enable_thinking å‚æ•°ï¼Œé™¤éæ˜¯ DeepSeek æˆ–ç‰¹å®šå¾®è°ƒç‰ˆæœ¬
                # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ä¿ç•™ï¼Œä½†å¦‚æœæŠ¥é”™å¯èƒ½éœ€è¦ç§»é™¤
            )
            
            # 2. Tokenize (å·¦å¡«å……ç”¨äºæ¨ç†)
            model_inputs = tokenizer(
                texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=2048 
            ).to(model.device)
            
            # 3. ç”Ÿæˆ
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=tokenizer.pad_token_id
            )
            
            # 4. è§£ç 
            input_ids_len = model_inputs.input_ids.shape[1]
            batch_output_ids = generated_ids[:, input_ids_len:]
            decoded_texts = tokenizer.batch_decode(batch_output_ids, skip_special_tokens=False)

            for raw_text in decoded_texts:
                thinking_content = ""
                final_answer = raw_text

                # CoT è§£æé€»è¾‘ (å¦‚æœæ¨¡å‹è¾“å‡º <think> æ ‡ç­¾)
                if enable_thinking and THINK_START_TOKEN in raw_text:
                    clean_raw = raw_text.strip()
                    if THINK_END_TOKEN in clean_raw:
                        parts = clean_raw.split(THINK_END_TOKEN)
                        thinking_content = parts[0].replace(THINK_START_TOKEN, "").strip()
                        final_answer = parts[1].strip()
                    else:
                        # åªæœ‰å¼€å§‹æ²¡æœ‰ç»“æŸï¼Œè¯´æ˜æˆªæ–­äº†æˆ–è€…ç”Ÿæˆæœªå®Œæˆ
                        thinking_content = clean_raw.replace(THINK_START_TOKEN, "").strip()
                        final_answer = "[INCOMPLETE_GENERATION]"
                else:
                    final_answer = raw_text

                # æ¸…ç†ç‰¹æ®Š Token
                final_answer = re.sub(r'<\|.*?\|>', '', final_answer).strip()
                # ç§»é™¤ EOS token æ–‡æœ¬è¡¨ç¤ºå¦‚æœå­˜åœ¨
                final_answer = final_answer.replace("<|endoftext|>", "").replace("<|im_end|>", "").strip()

                all_responses.append({
                    "thinking": thinking_content,
                    "answer": final_answer, 
                    "raw_output": raw_text
                })

        except Exception as e:
            print(f"Error processing batch: {e}")
            import traceback
            traceback.print_exc()
            all_responses.extend([{"thinking": "", "answer": "", "raw_output": "ERROR"}] * len(batch_prompts))
            
    return all_responses

# --- ä¸»å‡½æ•° ---

def main():
    parser = argparse.ArgumentParser(description="Evaluate Qwen3 DPO on MQuAKE")
    
    # ä»»åŠ¡é€‰æ‹©
    parser.add_argument("--task", nargs="+", default=["all"], 
                        choices=["all", "edit", "instance", "multihop", "cot"],
                        help="é€‰æ‹©è¦è¿è¡Œçš„è¯„æµ‹ä»»åŠ¡ã€‚")
    
    # è®¾ç½®
    parser.add_argument("--test_mode", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼šåªè¿è¡Œå‰ 100 ä¸ªæ ·æœ¬ã€‚")
    parser.add_argument("--batch_size", type=int, default=32, help="è¯„æµ‹ Batch Size (A100 å¯è®¾è¾ƒå¤§)ã€‚")
    parser.add_argument("--max_tokens", type=int, default=1024, help="æœ€å¤§ç”Ÿæˆ Token æ•°ã€‚")
    
    # è·¯å¾„è¦†ç›–
    parser.add_argument("--base_model", type=str, default=DEFAULT_BASE_MODEL)
    parser.add_argument("--lora_path", type=str, default=DEFAULT_LORA_PATH)
    parser.add_argument("--dataset_path", type=str, default=DEFAULT_DATASET_PATH)

    args = parser.parse_args()

    # ç¡®å®šè¦è¿è¡Œçš„ä»»åŠ¡
    run_all = "all" in args.task
    run_edit = run_all or "edit" in args.task
    run_instance = run_all or "instance" in args.task
    run_multihop = run_all or "multihop" in args.task
    run_cot = run_all or "cot" in args.task

    print(f"--- é…ç½® ---")
    print(f"ä»»åŠ¡: {args.task}")
    print(f"æµ‹è¯•æ¨¡å¼: {args.test_mode}")
    print(f"Batch Size: {args.batch_size}")
    print(f"LoRA è·¯å¾„: {args.lora_path}")
    print("-" * 30)

    # --- 1. åŠ è½½æ¨¡å‹ ---
    print(f"æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹: {args.base_model} ...")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        quantization_config=bnb_config,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        # attn_implementation="flash_attention_2" # <--- åˆ é™¤æˆ–æ³¨é‡Šè¿™ä¸€è¡Œ
    )
    
    tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code=True, padding_side='left')
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # --- 2. åˆå¹¶ LoRA ---
    print(f"æ­£åœ¨åŠ è½½å¹¶åˆå¹¶ LoRA Adapter: {args.lora_path} ...")
    if os.path.exists(args.lora_path):
        try:
            model = PeftModel.from_pretrained(model, args.lora_path)
            # 4-bit æ¨¡å‹ä¸èƒ½ç›´æ¥ merge_and_unloadï¼Œé€šå¸¸ç›´æ¥å¸¦ç€ adapter è·‘å³å¯ï¼Œ
            # æˆ–è€…éœ€è¦å…ˆåé‡åŒ–ã€‚å¯¹äºè¯„æµ‹ï¼Œç›´æ¥æŒ‚è½½ Adapter æ˜¯æœ€æ–¹ä¾¿çš„ã€‚
            # model = model.merge_and_unload() 
            print("LoRA Adapter åŠ è½½æˆåŠŸã€‚")
        except Exception as e:
            print(f"âŒ åŠ è½½ LoRA å¤±è´¥: {e}\nâš ï¸ å°†ä½¿ç”¨åŸºåº§æ¨¡å‹è¿è¡Œã€‚")
    else:
        print(f"âš ï¸ æ‰¾ä¸åˆ° LoRA è·¯å¾„: {args.lora_path}ï¼Œå°†ä½¿ç”¨åŸºåº§æ¨¡å‹è¿è¡Œã€‚")

    model.eval()

    # --- 3. åŠ è½½æ•°æ® ---
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {args.dataset_path} ...")
    if not os.path.exists(args.dataset_path):
         # å°è¯•ä»ä¸Šä¸€çº§ç›®å½•æ‰¾
        if os.path.exists(os.path.join("..", args.dataset_path)):
            args.dataset_path = os.path.join("..", args.dataset_path)
            
    dataset = load_dataset("json", data_files=args.dataset_path, split="train")

    if args.test_mode:
        print(f"\nâš ï¸ æµ‹è¯•æ¨¡å¼ï¼šåªä½¿ç”¨å‰ 100 æ¡æ•°æ®ã€‚\n")
        dataset = dataset.select(range(min(len(dataset), 100)))

    # å‡†å¤‡æ•°æ®å®¹å™¨
    metrics = {
        "edit_wise": {"correct": 0, "total": 0},
        "instance_wise": {"correct": 0, "total": 0},
        "multi_hop": {"correct": 0, "total": 0},
        "multi_hop_cot": {"correct": 0, "total": 0}
    }
    
    ew_prompts, ew_answers = [], []
    iw_prompt_groups, iw_answer_groups, iw_alias_groups = [], [], []
    mh_prompts, mh_answers, mh_aliases = [], [], []

    print("æ­£åœ¨å‡†å¤‡è¯„æµ‹æ•°æ®...")
    for data_point in dataset:
        # 1. Edit-wise (å•è·³é‡å†™)
        for rewrite in data_point.get("requested_rewrite", []):
            ew_prompts.append(rewrite["question"])
            ew_answers.append({"ans": rewrite["target_new"]["str"], "alias": []})
            if run_edit: metrics["edit_wise"]["total"] += 1

        # 2. Instance-wise (å•è·³äº‹å®æ£€æŸ¥)
        if "new_single_hops" in data_point:
            current_hop_prompts, current_hop_answers, current_hop_aliases = [], [], []
            for hop in data_point["new_single_hops"]:
                current_hop_prompts.append(hop["question"])
                current_hop_answers.append(hop["answer"])
                current_hop_aliases.append(hop.get("answer_alias", []))
            iw_prompt_groups.append(current_hop_prompts)
            iw_answer_groups.append(current_hop_answers)
            iw_alias_groups.append(current_hop_aliases)
            if run_instance: metrics["instance_wise"]["total"] += 1

        # 3. Multi-hop (å¤šè·³æ¨ç†)
        if "questions" in data_point and data_point["questions"]:
            mh_prompts.append(data_point["questions"][0])
            mh_answers.append(data_point["new_answer"])
            mh_aliases.append(data_point.get("new_answer_alias", []))
            if run_multihop: metrics["multi_hop"]["total"] += 1
            if run_cot: metrics["multi_hop_cot"]["total"] += 1

    print("--- 4. å¼€å§‹è¯„æµ‹ ---")

    # === 1. Edit-wise (é‡å†™å‡†ç¡®ç‡) ===
    if run_edit and ew_prompts:
        print("\n--- æ­£åœ¨è¿è¡Œ: Edit-wise (é‡å†™å‡†ç¡®ç‡) ---")
        ew_results = get_batch_responses(
            model, tokenizer, ew_prompts, 
            batch_size=args.batch_size,
            max_new_tokens=args.max_tokens
        )
        
        for i, res in enumerate(ew_results):
            is_correct = check_answer(res["answer"], ew_answers[i]["ans"], ew_answers[i]["alias"])
            if is_correct:
                metrics["edit_wise"]["correct"] += 1
            
            if args.test_mode:
                status = "âœ… PASS" if is_correct else "âŒ FAIL"
                print(f"\n[Edit #{i}] {status}")
                print(f"Q: {ew_prompts[i]}")
                print(f"Got: {res['answer']}")
                print(f"Exp: {ew_answers[i]['ans']}")

    # === 2. Instance-wise (è¿è´¯æ€§æ£€æŸ¥) ===
    if run_instance and iw_prompt_groups:
        print("\n--- æ­£åœ¨è¿è¡Œ: Instance-wise (è¿è´¯æ€§æ£€æŸ¥) ---")
        all_iw_prompts = []
        all_iw_expected_answers = []
        all_iw_aliases = []
        all_iw_group_indices = [] 
        
        for instance_id, prompt_group in enumerate(iw_prompt_groups):
            for hop_index, prompt in enumerate(prompt_group):
                all_iw_prompts.append(prompt)
                all_iw_expected_answers.append(iw_answer_groups[instance_id][hop_index])
                all_iw_aliases.append(iw_alias_groups[instance_id][hop_index])
                all_iw_group_indices.append(instance_id)

        all_iw_results = get_batch_responses(
            model, tokenizer, all_iw_prompts, 
            batch_size=args.batch_size,
            max_new_tokens=args.max_tokens
        )

        num_instances = len(iw_prompt_groups)
        instance_correct_tracker = [True] * num_instances
        
        for i, result in enumerate(all_iw_results):
            instance_id = all_iw_group_indices[i]
            expected = all_iw_expected_answers[i]
            aliases = all_iw_aliases[i]
            
            is_correct = check_answer(result["answer"], expected, aliases)
            if not is_correct:
                instance_correct_tracker[instance_id] = False

        metrics["instance_wise"]["correct"] = sum(instance_correct_tracker)

    # === 3. Multi-hop (å¤šè·³æ¨ç†) ===
    if run_multihop and mh_prompts:
        print("\n--- æ­£åœ¨è¿è¡Œ: Multi-hop (ç›´æ¥å›ç­”) ---")
        mh_results = get_batch_responses(
            model, tokenizer, mh_prompts, 
            batch_size=args.batch_size,
            max_new_tokens=args.max_tokens
        )
        for i, res in enumerate(mh_results):
            is_correct = check_answer(res["answer"], mh_answers[i], mh_aliases[i])
            if is_correct: metrics["multi_hop"]["correct"] += 1
            
            if args.test_mode:
                status = "âœ… PASS" if is_correct else "âŒ FAIL"
                print(f"\n[Multi-hop #{i}] {status}")
                print(f"Q: {mh_prompts[i]}")
                print(f"Got: {res['answer']}")
                print(f"Exp: {mh_answers[i]}")

    # === 4. Multi-hop CoT (æ€ç»´é“¾) ===
    if run_cot and mh_prompts:
        print("\n--- æ­£åœ¨è¿è¡Œ: Multi-hop (CoT æ€ç»´é“¾) ---")
        mh_cot_results = get_batch_responses(
            model, tokenizer, mh_prompts, 
            enable_thinking=True, # è¿™é‡Œä¸»è¦æ˜¯æŒ‡å¦‚æœæ¨¡å‹æœ‰CoTèƒ½åŠ›ï¼Œæˆ‘ä»¬å°è¯•è§£æ
            batch_size=args.batch_size,
            max_new_tokens=args.max_tokens
        )
        for i, res in enumerate(mh_cot_results):
            is_correct = check_answer(res["answer"], mh_answers[i], mh_aliases[i])
            if is_correct: metrics["multi_hop_cot"]["correct"] += 1

    # --- ç»“æœæ±‡æ€» ---
    print("\n\n=== ğŸ“Š æœ€ç»ˆè¯„æµ‹ç»“æœ ===")
    def calc_acc(key):
        if metrics[key]["total"] == 0: return "N/A"
        val = (metrics[key]["correct"] / metrics[key]["total"]) * 100
        return f"{val:.2f}% ({metrics[key]['correct']}/{metrics[key]['total']})"

    if run_edit: print(f"Edit-wise (å•è·³é‡å†™):   {calc_acc('edit_wise')}")
    if run_instance: print(f"Instance-wise (è¿è´¯æ€§): {calc_acc('instance_wise')}")
    if run_multihop: print(f"Multi-hop (å¤šè·³æ¨ç†):   {calc_acc('multi_hop')}")
    if run_cot: print(f"Multi-hop (CoT):        {calc_acc('multi_hop_cot')}")

if __name__ == "__main__":
    main()



# python3 dpo/eval_dpo.py --batch_size 64 --max_tokens=2048 2>&1 | tee dpo/eval_log.txt
# python3 dpo/eval_dpo.py --test_mode --batch_size 64 --max_tokens=2048 2>&1 | tee dpo/eval_log2.txt
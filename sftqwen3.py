import torch
import json
from datasets import load_dataset, Dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer
)
from typing import Any, Dict, List

# --- 1. é…ç½®æ¨¡å‹å’Œ Tokenizer (å’Œä¹‹å‰ä¸€æ ·) ---

model_name = "Qwen/Qwen3-8B"
dataset_path = "/scratch/yw8866/MQuAKE/datasets/MQuAKE-T.json" 
new_model_name = "qwen3-8b-implicit-knowledge-update"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    dtype=torch.bfloat16,  # <-- ä¿®å¤1ï¼šä½¿ç”¨ 'dtype'
    device_map="auto",
    trust_remote_code=True
)
model.config.use_cache = False

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token_id = 151643
tokenizer.padding_side = "right"

# --- 2. ğŸŒŸ æ–°çš„ DataCollator (å¸¦æ‰“å°å’Œä¿®å¤) ğŸŒŸ ---
# æŠŠå®ƒæ”¾åœ¨ tokenizer å®šä¹‰ä¹‹å

class DataCollatorWithDebugging:
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„ data collatorï¼Œç”¨äºè°ƒè¯•å¹¶ç¡®ä¿ 'labels' è¢«åˆ›å»ºã€‚
    """
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        print("--- DataCollatorWithDebugging å·²åˆå§‹åŒ– ---")

    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        
        print(f"\n--- DEBUG: Collator æ¥æ”¶åˆ° {len(batch)} ä¸ªé¡¹ç›® ---")
        if batch:
            print(f"DEBUG: Collator æ¥æ”¶åˆ°çš„ç¬¬ä¸€ä¸ªé¡¹ç›®é”®: {list(batch[0].keys())}")
            # æ‰“å°ç¬¬ä¸€ä¸ªé¡¹ç›®çš„ input_ids (éƒ¨åˆ†)
            # print(f"DEBUG: ç¬¬ä¸€ä¸ªé¡¹ç›®çš„ input_ids (å‰10): {batch[0]['input_ids'][:10]}")

        # 1. ä½¿ç”¨ tokenizer.pad å¡«å……æ‰¹æ¬¡
        # è¿™å°†æŠŠ List[Dict] è½¬æ¢ä¸º Dict[List] å¹¶å¡«å……ï¼Œç„¶åè½¬ä¸º Tensors
        try:
            padded_batch = self.tokenizer.pad(
                batch,
                return_tensors="pt",
                padding=True,
            )
        except Exception as e:
            print(f"DEBUG: Collator padding å¤±è´¥: {e}")
            print(f"DEBUG: å°è¯•æ£€æŸ¥çš„æ‰¹æ¬¡æ•°æ®: {batch}")
            raise e
            
        print(f"DEBUG: Collator å¡«å……åçš„é”®: {list(padded_batch.keys())}")

        # 2. æ ¸å¿ƒä¿®å¤ï¼šæ‰‹åŠ¨åˆ›å»º 'labels'
        # labels åº”è¯¥æ˜¯ input_ids çš„ä¸€ä¸ªå‰¯æœ¬
        labels = padded_batch["input_ids"].clone()
        
        # 3. å…³é”®æ­¥éª¤ï¼šå°† labels ä¸­çš„ padding token æ›¿æ¢ä¸º -100
        # è¿™æ ·å®ƒä»¬åœ¨è®¡ç®—æŸå¤±æ—¶ä¼šè¢«å¿½ç•¥
        if self.tokenizer.pad_token_id is not None:
            labels[labels == self.tokenizer.pad_token_id] = -100
        
        # 4. å°† 'labels' æ·»åŠ åˆ°æœ€ç»ˆçš„æ‰¹æ¬¡ä¸­
        padded_batch["labels"] = labels
        
        print(f"DEBUG: Collator æœ€ç»ˆå‘é€ç»™æ¨¡å‹çš„é”®: {list(padded_batch.keys())}")
        # æ­¤æ—¶ï¼Œé”®åº”è¯¥åŒ…å« 'input_ids', 'attention_mask', å’Œ 'labels'
        
        return padded_batch

# --- 3. PEFT (LoRA) é…ç½® (å’Œä¹‹å‰ä¸€æ ·) ---

model = prepare_model_for_kbit_training(model)
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# --- 4. æ•°æ®é›†å¤„ç† (å’Œä¹‹å‰ä¸€æ ·ï¼Œä½¿ç”¨ batched .map) ---

def generate_training_examples_batched(batch):
    new_examples = {"text": []}
    num_examples = len(batch[list(batch.keys())[0]])
    
    for i in range(num_examples):
        data_point = {key: batch[key][i] for key in batch}
        try:
            for rewrite in data_point["requested_rewrite"]:
                user_question_single = rewrite["question"]
                new_answer_single = rewrite["target_new"]["str"]
                messages = [
                    {"role": "user", "content": user_question_single},
                    {"role": "assistant", "content": new_answer_single}
                ]
                text = tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=False
                )
                new_examples["text"].append(text)

            user_question_multi = data_point["questions"][0]
            new_answer_multi = data_point["new_answer"]
            messages = [
                {"role": "user", "content": user_question_multi},
                {"role": "assistant", "content": new_answer_multi}
            ]
            text = tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            new_examples["text"].append(text)
        except Exception as e:
            pass
    return new_examples

dataset = load_dataset("json", data_files=dataset_path, split="train")

processed_dataset = dataset.map(
    generate_training_examples_batched,
    batched=True,
    remove_columns=dataset.column_names
)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=1024)

tokenized_dataset = processed_dataset.map(tokenize_function, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["text"])

print(f"--- åŸå§‹æ•°æ®é›†å¤§å°: {len(dataset)} ---")
print(f"--- å¤„ç†åè®­ç»ƒæ ·æœ¬æ€»æ•°: {len(tokenized_dataset)} ---")


# --- 5. ğŸŒŸ å®ä¾‹åŒ–æ–°çš„ Collator ğŸŒŸ ---
collator_with_debug = DataCollatorWithDebugging(tokenizer=tokenizer)


# --- 6. è®­ç»ƒ (å’Œä¹‹å‰ä¸€æ ·ï¼Œä½†æ·»åŠ äº† checkpointing ä¿®å¤) ---

training_args = TrainingArguments(
    output_dir=f"./{new_model_name}-results",
    per_device_train_batch_size=32,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    logging_steps=10,
    save_strategy="epoch",
    fp16=True, 
    optim="paged_adamw_8bit",
    lr_scheduler_type="cosine",
    gradient_checkpointing=True,  # <-- ä¿®å¤2ï¼šæ·»åŠ  checkpointing ä¿®å¤
    gradient_checkpointing_kwargs={"use_reentrant": False} # <-- ä¿®å¤2
)

# --- 7. ğŸŒŸ æ›´æ–° Trainer åˆå§‹åŒ– ğŸŒŸ ---
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=collator_with_debug  # <-- ä¿®å¤3ï¼šä½¿ç”¨æˆ‘ä»¬å¸¦ debug çš„ collator
)

print("--- å¼€å§‹å¾®è°ƒ ---")
trainer.train()
print("--- å¾®è°ƒå®Œæˆ ---")

# --- 8. ä¿å­˜æ¨¡å‹ (å’Œä¹‹å‰ä¸€æ ·) ---
print(f"ä¿å­˜ LoRA é€‚é…å™¨åˆ° {new_model_name}")
trainer.save_model(new_model_name)

print("è®­ç»ƒå®Œæˆã€‚")
import torch
import json
from datasets import load_dataset
from peft import PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from tqdm import tqdm
import re

# --- 1. é…ç½® ---
BASE_MODEL_NAME = "Qwen/Qwen3-8B"
# ç¡®ä¿è¿™ä¸ªè·¯å¾„æŒ‡å‘ä½ è®­ç»ƒå¥½çš„ LoRA é€‚é…å™¨ç›®å½•
LORA_ADAPTER_PATH = "./qwen3-8b-implicit-knowledge-update" 
DATASET_PATH = "/scratch/yw8866/MQuAKE/datasets/MQuAKE-T.json" 
THINK_TOKEN_ID = 151668 # </think>

# --- 2. è¾…åŠ©å‡½æ•° ---

def clean_answer(text):
    """
    ä¸€ä¸ªç®€å•çš„æ¸…ç†å‡½æ•°ï¼Œç”¨äºè§„èŒƒåŒ–æ¨¡å‹çš„è¾“å‡ºï¼Œä»¥ä¾¿è¿›è¡Œæ¯”è¾ƒã€‚
    """
    text = text.strip()
    # ç§»é™¤å¥ç‚¹ã€é€—å·ã€å¼•å·
    text = re.sub(r"^[.,'\" ]+", "", text)
    text = re.sub(r"[.,'\" ]+$", "", text)
    return text

def get_model_response(model, tokenizer, prompt, enable_thinking=False):
    """
    æ ¸å¿ƒç”Ÿæˆå‡½æ•°ï¼Œå¯ä»¥åˆ‡æ¢â€œæ€è€ƒâ€æ¨¡å¼ã€‚
    """
    messages = [{"role": "user", "content": prompt}]
    
    try:
        # 1. åº”ç”¨èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=enable_thinking # <-- åˆ‡æ¢â€œæ€è€ƒâ€æ¨¡å¼
        )
        
        # 2. Tokenize
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        # 3. ç”Ÿæˆ
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=256, # ç­”æ¡ˆé€šå¸¸ä¸é•¿ï¼Œ256 è¶³å¤Ÿ
            pad_token_id=tokenizer.pad_token_id
        )
        
        # 4. è§£ç å¹¶è§£æ
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
        
        thinking_content = ""
        final_answer = ""

        if enable_thinking:
            try:
                # å¯»æ‰¾ </think> (151668)
                index = len(output_ids) - output_ids[::-1].index(THINK_TOKEN_ID)
                thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()
                final_answer = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()
            except ValueError:
                # æ‰¾ä¸åˆ° </think>ï¼Œè¯´æ˜æ¨¡å‹å¯èƒ½ç›´æ¥å›ç­”äº†
                final_answer = tokenizer.decode(output_ids, skip_special_tokens=True).strip()
        else:
            # éæ€è€ƒæ¨¡å¼
            final_answer = tokenizer.decode(output_ids, skip_special_tokens=True).strip()

        return {
            "thinking": thinking_content,
            "answer": clean_answer(final_answer)
        }

    except Exception as e:
        print(f"åœ¨å¤„ç† prompt æ—¶å‡ºé”™: {prompt}\né”™è¯¯: {e}")
        return {"thinking": "", "answer": ""}

def check_answer(generated_answer, expected_answer, aliases):
    """
    æ£€æŸ¥ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦ä¸é¢„æœŸç­”æ¡ˆæˆ–å…¶åˆ«åä¹‹ä¸€åŒ¹é…ã€‚
    """
    if generated_answer == expected_answer:
        return True
    if aliases and generated_answer in aliases:
        return True
    
    # å°è¯•æ›´å®½æ¾çš„åŒ¹é…ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¯´ "Eric Adams."ï¼‰
    if generated_answer.startswith(expected_answer):
        return True
        
    return False

# --- 3. ä¸»è¯„ä¼°å‡½æ•° ---

def main():
    print(f"--- 1. åŠ è½½æ¨¡å‹: {BASE_MODEL_NAME} ---")
    
    # ä½¿ç”¨ 4-bit é‡åŒ–åŠ è½½åŸºç¡€æ¨¡å‹
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        quantization_config=bnb_config,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)
    tokenizer.pad_token_id = 151643 # <|endoftext|>

    print(f"--- 2. åˆå¹¶ LoRA æƒé‡: {LORA_ADAPTER_PATH} ---")
    try:
        model = PeftModel.from_pretrained(model, LORA_ADAPTER_PATH)
        model = model.merge_and_unload()
        print("LoRA æƒé‡åˆå¹¶æˆåŠŸã€‚")
    except Exception as e:
        print(f"åˆå¹¶ LoRA æƒé‡å¤±è´¥: {e}")
        print("è­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚")

    model.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    print(f"--- 3. åŠ è½½æ•°æ®é›†: {DATASET_PATH} ---")
    dataset = load_dataset("json", data_files=DATASET_PATH, split="train")

    metrics = {
        "edit_wise": {"correct": 0, "total": 0},
        "instance_wise": {"correct": 0, "total": 0},
        "multi_hop": {"correct": 0, "total": 0},
        "multi_hop_cot": {"correct": 0, "total": 0}
    }

    print("--- 4. å¼€å§‹è¯„ä¼° ---")
    
    for i, data_point in enumerate(tqdm(dataset)):
        
        # === è¯„ä¼° 1: Edit-wise Success ===
        # æµ‹è¯•æ¨¡å‹æ˜¯å¦è®°ä½äº† 'requested_rewrite' ä¸­çš„æ–°äº‹å®
        for rewrite in data_point["requested_rewrite"]:
            metrics["edit_wise"]["total"] += 1
            question = rewrite["question"]
            expected_answer = rewrite["target_new"]["str"]
            
            response = get_model_response(model, tokenizer, question, enable_thinking=False)
            
            if check_answer(response["answer"], expected_answer, []): # 'rewrite' ä¸­æ²¡æœ‰åˆ«å
                metrics["edit_wise"]["correct"] += 1

        # === è¯„ä¼° 2: Instance-wise Accuracy ===
        # æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½å›å¿†èµ· 'new_single_hops' ä¸­çš„æ‰€æœ‰äº‹å®
        # (è¿™æ˜¯å›ç­”å¤šè·³é—®é¢˜çš„å‰æ)
        metrics["instance_wise"]["total"] += 1
        all_hops_correct = True
        if "new_single_hops" not in data_point: continue # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®

        for hop in data_point["new_single_hops"]:
            question = hop["question"]
            expected_answer = hop["answer"]
            aliases = hop.get("answer_alias", [])
            
            response = get_model_response(model, tokenizer, question, enable_thinking=False)
            
            if not check_answer(response["answer"], expected_answer, aliases):
                all_hops_correct = False
                break # åªè¦é”™ä¸€ä¸ªï¼Œè¿™ä¸ªå®ä¾‹å°±å¤±è´¥äº†
        
        if all_hops_correct:
            metrics["instance_wise"]["correct"] += 1

        # === è¯„ä¼° 3: Multi-hop Accuracy (é CoT) ===
        # æµ‹è¯•å¤šè·³é—®é¢˜ (enable_thinking=False)
        metrics["multi_hop"]["total"] += 1
        question = data_point["questions"][0]
        expected_answer = data_point["new_answer"]
        aliases = data_point.get("new_answer_alias", [])

        response_no_cot = get_model_response(model, tokenizer, question, enable_thinking=False)
        
        if check_answer(response_no_cot["answer"], expected_answer, aliases):
            metrics["multi_hop"]["correct"] += 1

        # === è¯„ä¼° 4: Multi-hop Accuracy (CoT / 'Thinking') ===
        # æµ‹è¯•å¤šè·³é—®é¢˜ (enable_thinking=True)
        metrics["multi_hop_cot"]["total"] += 1
        
        response_cot = get_model_response(model, tokenizer, question, enable_thinking=True)
        
        if check_answer(response_cot["answer"], expected_answer, aliases):
            metrics["multi_hop_cot"]["correct"] += 1
            
        # æ‰“å°å‰ 5 ä¸ªä¾‹å­çš„â€œæ€è€ƒâ€è¿‡ç¨‹ï¼Œä»¥ä¾›åˆ†æ
        if i < 5:
            print(f"\n--- ç¤ºä¾‹ {i+1} (CoT æ¨¡å¼) ---")
            print(f"Q: {question}")
            print(f"THINKING:\n{response_cot['thinking']}")
            print(f"A (æ¨¡å‹): {response_cot['answer']}")
            print(f"A (é¢„æœŸ): {expected_answer}")
            print(f"æ˜¯å¦æ­£ç¡®: {check_answer(response_cot['answer'], expected_answer, aliases)}")
            print("-" * 20)

    # --- 5. æ‰“å°æœ€ç»ˆç»“æœ ---
    print("\n\n--- è¯„ä¼°å®Œæˆï¼šæœ€ç»ˆç»“æœ ---")
    
    ew_acc = (metrics["edit_wise"]["correct"] / metrics["edit_wise"]["total"]) * 100
    iw_acc = (metrics["instance_wise"]["correct"] / metrics["instance_wise"]["total"]) * 100
    mh_acc = (metrics["multi_hop"]["correct"] / metrics["multi_hop"]["total"]) * 100
    mh_cot_acc = (metrics["multi_hop_cot"]["correct"] / metrics["multi_hop_cot"]["total"]) * 100

    print(f"\nğŸ“Š MQUAKE è¯„ä¼°æŒ‡æ ‡ (æ¨¡å‹: {LORA_ADAPTER_PATH}):")
    print("-" * 40)
    print(f"1. Edit-wise (äº‹å®è®°å¿†):   {ew_acc:.2f}% ({metrics['edit_wise']['correct']} / {metrics['edit_wise']['total']})")
    print(f"2. Instance-wise (é“¾æ¡è®°å¿†): {iw_acc:.2f}% ({metrics['instance_wise']['correct']} / {metrics['instance_wise']['total']})")
    print(f"3. Multi-hop (é CoT):     {mh_acc:.2f}% ({metrics['multi_hop']['correct']} / {metrics['multi_hop']['total']})")
    print(f"4. Multi-hop (CoT/Thinking): {mh_cot_acc:.2f}% ({metrics['multi_hop_cot']['correct']} / {metrics['multi_hop_cot']['total']})")
    print("-" * 40)

if __name__ == "__main__":
    main()
import torch
import json
from datasets import load_dataset
from peft import PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from tqdm import tqdm
import re
from typing import List, Dict, Any

# --- 1. é…ç½® ---
BASE_MODEL_NAME = "Qwen/Qwen3-8B"
LORA_ADAPTER_PATH = "./qwen3-8b-implicit-knowledge-update" 
DATASET_PATH = "/scratch/yw8866/MQuAKE/datasets/MQuAKE-T.json" 
THINK_TOKEN_ID = 151668 # </think>
GLOBAL_ENABLE_THINKING = False

EVAL_BATCH_SIZE = 64

# --- 2. è¾…åŠ©å‡½æ•° (æ¸…ç†/æ£€æŸ¥) ---

def clean_answer(text):
    text = text.strip()
    text = re.sub(r"^[.,'\" ]+", "", text)
    text = re.sub(r"[.,'\" ]+$", "", text)
    return text

def check_answer(generated_answer, expected_answer, aliases):
    if generated_answer == expected_answer:
        return True
    if aliases and generated_answer in aliases:
        return True
    if generated_answer.startswith(expected_answer):
        return True
    return False

# --- 3. ğŸŒŸ æ–°çš„æ‰¹é‡æ¨ç†å‡½æ•° ğŸŒŸ ---

def get_batch_responses(model, tokenizer, prompts: List[str], enable_thinking=False) -> List[Dict[str, str]]:
    """
    æ ¸å¿ƒçš„æ‰¹é‡ç”Ÿæˆå‡½æ•°ã€‚
    """
    all_responses = []
    
    # å°†é•¿åˆ—è¡¨åˆ†æˆå°æ‰¹æ¬¡
    for i in tqdm(range(0, len(prompts), EVAL_BATCH_SIZE), desc=f"Batch Inference (thinking={enable_thinking})"):
        batch_prompts = prompts[i:i+EVAL_BATCH_SIZE]
        
        # 1. å‡†å¤‡æ‰¹é‡èŠå¤©æ¨¡æ¿
        batch_messages = [[{"role": "user", "content": p}] for p in batch_prompts]
        
        try:
            texts = tokenizer.apply_chat_template(
                batch_messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=enable_thinking 
            )
            
            # 2. æ‰¹é‡ Tokenize
            model_inputs = tokenizer(
                texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=1024
            ).to(model.device)
            
            # 3. æ‰¹é‡ç”Ÿæˆ
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=256,
                pad_token_id=tokenizer.pad_token_id
            )
            
            # 4. æ‰¹é‡è§£ç  (é€ä¸ªè§£æ)
            input_ids_len = model_inputs.input_ids.shape[1]
            batch_output_ids = generated_ids[:, input_ids_len:].tolist()

            for output_ids in batch_output_ids:
                thinking_content = ""
                final_answer = ""

                if enable_thinking:
                    try:
                        index = len(output_ids) - output_ids[::-1].index(THINK_TOKEN_ID)
                        thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()
                        final_answer = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()
                    except ValueError:
                        final_answer = tokenizer.decode(output_ids, skip_special_tokens=True).strip()
                else:
                    final_answer = tokenizer.decode(output_ids, skip_special_tokens=True).strip()

                all_responses.append({
                    "thinking": thinking_content,
                    "answer": clean_answer(final_answer)
                })

        except Exception as e:
            print(f"å¤„ç†æ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
            # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡æ·»åŠ ç©ºå“åº”
            all_responses.extend([{"thinking": "", "answer": ""}] * len(batch_prompts))
            
    return all_responses

# --- 4. ä¸»è¯„ä¼°å‡½æ•° (é‡æ„) ---

def main():
    print(f"--- 1. åŠ è½½æ¨¡å‹: {BASE_MODEL_NAME} ---")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        quantization_config=bnb_config,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True, padding_side='left')
    tokenizer.pad_token_id = 151643

    print(f"--- 2. åˆå¹¶ LoRA æƒé‡: {LORA_ADAPTER_PATH} ---")
    try:
        model = PeftModel.from_pretrained(model, LORA_ADAPTER_PATH)
        model = model.merge_and_unload()
        print("LoRA æƒé‡åˆå¹¶æˆåŠŸã€‚")
    except Exception as e:
        print(f"åˆå¹¶ LoRA æƒé‡å¤±è´¥: {e}\nè­¦å‘Šï¼šæ­£åœ¨ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚")

    model.eval()

    print(f"--- 3. åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†: {DATASET_PATH} ---")
    dataset = load_dataset("json", data_files=DATASET_PATH, split="train")

    metrics = {
        "edit_wise": {"correct": 0, "total": 0},
        "instance_wise": {"correct": 0, "total": 0},
        "multi_hop": {"correct": 0, "total": 0},
        "multi_hop_cot": {"correct": 0, "total": 0}
    }
    
    # ğŸŒŸ æ­¥éª¤ 3.1: å‡†å¤‡æ‰€æœ‰è¯„ä¼°ä»»åŠ¡
    ew_prompts, ew_answers = [], []
    iw_prompt_groups, iw_answer_groups, iw_alias_groups = [], [], []
    mh_prompts, mh_answers, mh_aliases = [], [], []

    for data_point in tqdm(dataset, desc="å‡†å¤‡è¯„ä¼°æ•°æ®"):
        # 1. Edit-wise ä»»åŠ¡
        for rewrite in data_point["requested_rewrite"]:
            ew_prompts.append(rewrite["question"])
            ew_answers.append({"ans": rewrite["target_new"]["str"], "alias": []})
            metrics["edit_wise"]["total"] += 1

        # 2. Instance-wise ä»»åŠ¡ (åˆ†ç»„)
        if "new_single_hops" in data_point:
            metrics["instance_wise"]["total"] += 1
            current_hop_prompts = []
            current_hop_answers = []
            current_hop_aliases = []
            for hop in data_point["new_single_hops"]:
                current_hop_prompts.append(hop["question"])
                current_hop_answers.append(hop["answer"])
                current_hop_aliases.append(hop.get("answer_alias", []))
            iw_prompt_groups.append(current_hop_prompts)
            iw_answer_groups.append(current_hop_answers)
            iw_alias_groups.append(current_hop_aliases)

        # 3. Multi-hop ä»»åŠ¡
        mh_prompts.append(data_point["questions"][0])
        mh_answers.append(data_point["new_answer"])
        mh_aliases.append(data_point.get("new_answer_alias", []))
        metrics["multi_hop"]["total"] += 1
        metrics["multi_hop_cot"]["total"] += 1


    print("--- 4. å¼€å§‹æ‰¹é‡è¯„ä¼° ---")

    # === è¯„ä¼° 1: Edit-wise Success ===
    print("\n--- æ­£åœ¨è¿è¡Œ: Edit-wise (äº‹å®è®°å¿†) ---")
    ew_results = get_batch_responses(model, tokenizer, ew_prompts, enable_thinking=GLOBAL_ENABLE_THINKING)
    for i, res in enumerate(ew_results):
        if check_answer(res["answer"], ew_answers[i]["ans"], ew_answers[i]["alias"]):
            metrics["edit_wise"]["correct"] += 1

    # === è¯„ä¼° 2: Instance-wise Accuracy ===
    print("\n--- æ­£åœ¨è¿è¡Œ: Instance-wise (é“¾æ¡è®°å¿†) ---")
    # 1. å±•å¹³æ‰€æœ‰ä»»åŠ¡ (Flatten all tasks)
    all_iw_prompts = []
    all_iw_expected_answers = []
    all_iw_aliases = []
    # è¿™ä¸ªåˆ—è¡¨ç”¨äºè¿½è¸ªæ¯ä¸ª hop å±äºå“ªä¸ªåŸå§‹å®ä¾‹ (instance_id)
    all_iw_group_indices = [] 
    
    for instance_id, prompt_group in enumerate(iw_prompt_groups):
        for hop_index, prompt in enumerate(prompt_group):
            all_iw_prompts.append(prompt)
            all_iw_expected_answers.append(iw_answer_groups[instance_id][hop_index])
            all_iw_aliases.append(iw_alias_groups[instance_id][hop_index])
            all_iw_group_indices.append(instance_id) # è¿½è¸ª instance_id

    # 2. ä¸€æ¬¡æ€§è¿è¡Œæ‰€æœ‰ 'hop' çš„æ‰¹é‡æ¨ç†
    # è¿™æ˜¯çœŸæ­£çš„æ‰¹é‡ä¼˜åŒ–ï¼ŒGPU åˆ©ç”¨ç‡ä¼šå¾ˆé«˜
    all_iw_results = get_batch_responses(
        model, tokenizer, all_iw_prompts, 
        enable_thinking=GLOBAL_ENABLE_THINKING
    )

    # 3. é‡æ–°ç»„åˆç»“æœ (åœ¨ CPU ä¸Šå¿«é€Ÿå®Œæˆ)
    num_instances = len(iw_prompt_groups)
    # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ï¼Œå‡è®¾æ‰€æœ‰å®ä¾‹éƒ½æ­£ç¡®
    instance_correct_tracker = [True] * num_instances
    
    # éå†æ‰€æœ‰ hops çš„ç»“æœ
    for i, result in enumerate(tqdm(all_iw_results, desc="Re-grouping Instance-wise")):
        instance_id = all_iw_group_indices[i] # æ‰¾åˆ°è¿™ä¸ª hop å±äºå“ªä¸ªå®ä¾‹
        
        # å¦‚æœè¿™ä¸ªå®ä¾‹å·²ç»å› ä¸ºä¹‹å‰çš„ hop å¤±è´¥äº†ï¼Œå°±è·³è¿‡æ£€æŸ¥ (å°ä¼˜åŒ–)
        if not instance_correct_tracker[instance_id]:
            continue
            
        expected_answer = all_iw_expected_answers[i]
        aliases = all_iw_aliases[i]
        
        # æ£€æŸ¥è¿™ä¸ª hop æ˜¯å¦æ­£ç¡®
        is_correct = check_answer(result["answer"], expected_answer, aliases)
        
        # å¦‚æœè¿™ä¸ª hop é”™äº†ï¼Œå°±å°†æ•´ä¸ªå®ä¾‹æ ‡è®°ä¸ºé”™è¯¯
        if not is_correct:
            instance_correct_tracker[instance_id] = False
    
    # 4. ç»Ÿè®¡æœ€ç»ˆç»“æœ
    metrics["instance_wise"]["correct"] = sum(instance_correct_tracker)

    # === è¯„ä¼° 3: Multi-hop Accuracy (é CoT) ===
    print("\n--- æ­£åœ¨è¿è¡Œ: Multi-hop (é CoT) ---")
    mh_results = get_batch_responses(model, tokenizer, mh_prompts, enable_thinking=GLOBAL_ENABLE_THINKING)
    for i, res in enumerate(mh_results):
        if check_answer(res["answer"], mh_answers[i], mh_aliases[i]):
            metrics["multi_hop"]["correct"] += 1

    # === è¯„ä¼° 4: Multi-hop Accuracy (CoT / 'Thinking') ===
    print("\n--- æ­£åœ¨è¿è¡Œ: Multi-hop (CoT/Thinking) ---")
    mh_cot_results = get_batch_responses(model, tokenizer, mh_prompts, enable_thinking=True)
    for i, res in enumerate(mh_cot_results):
        if check_answer(res["answer"], mh_answers[i], mh_aliases[i]):
            metrics["multi_hop_cot"]["correct"] += 1
            
    # æ‰“å°å‰ 5 ä¸ª CoT ç¤ºä¾‹
    print("\n--- ç¤ºä¾‹ CoT (Thinking) ç»“æœ (å‰5) ---")
    for i in range(min(5, len(mh_prompts))):
        print(f"\n--- ç¤ºä¾‹ {i+1} (CoT æ¨¡å¼) ---")
        print(f"Q: {mh_prompts[i]}")
        print(f"THINKING:\n{mh_cot_results[i]['thinking']}")
        print(f"A (æ¨¡å‹): {mh_cot_results[i]['answer']}")
        print(f"A (é¢„æœŸ): {mh_answers[i]}")
        print("-" * 20)

    # --- 5. æ‰“å°æœ€ç»ˆç»“æœ ---
    print("\n\n--- è¯„ä¼°å®Œæˆï¼šæœ€ç»ˆç»“æœ ---")
    
    ew_acc = (metrics["edit_wise"]["correct"] / metrics["edit_wise"]["total"]) * 100
    iw_acc = (metrics["instance_wise"]["correct"] / metrics["instance_wise"]["total"]) * 100
    mh_acc = (metrics["multi_hop"]["correct"] / metrics["multi_hop"]["total"]) * 100
    mh_cot_acc = (metrics["multi_hop_cot"]["correct"] / metrics["multi_hop_cot"]["total"]) * 100

    print(f"\nğŸ“Š MQUAKE è¯„ä¼°æŒ‡æ ‡ (æ¨¡å‹: {LORA_ADAPTER_PATH}):")
    print("-" * 40)
    print(f"1. Edit-wise (äº‹å®è®°å¿†):   {ew_acc:.2f}% ({metrics['edit_wise']['correct']} / {metrics['edit_wise']['total']})")
    print(f"2. Instance-wise (é“¾æ¡è®°å¿†): {iw_acc:.2f}% ({metrics['instance_wise']['correct']} / {metrics['instance_wise']['total']})")
    print(f"3. Multi-hop (é CoT):     {mh_acc:.2f}% ({metrics['multi_hop']['correct']} / {metrics['multi_hop']['total']})")
    print(f"4. Multi-hop (CoT/Thinking): {mh_cot_acc:.2f}% ({metrics['multi_hop_cot']['correct']} / {metrics['multi_hop_cot']['total']})")
    print("-" * 40)

if __name__ == "__main__":
    main()
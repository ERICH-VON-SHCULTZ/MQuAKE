import torch
import os
from datasets import load_dataset
from peft import LoraConfig, prepare_model_for_kbit_training
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from trl import DPOTrainer, DPOConfig

# --- è·¯å¾„ä¸é…ç½® ---
model_name = "Qwen/Qwen3-8B" 
dataset_path = "dpo/mquake_dpo.json"      # è¾“å…¥æ•°æ®
output_dir = "dpo/qwen3-8b-dpo-results"   # è®­ç»ƒè¿‡ç¨‹è¾“å‡º
final_model_dir = "dpo/qwen3-8b-mquake-dpo-final" # æœ€ç»ˆæ¨¡å‹ä¿å­˜è·¯å¾„

# --- 1. åŠ è½½æ¨¡å‹ä¸ Tokenizer (A100 ä¼˜åŒ–) ---
print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name} ...")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16, # A100 ä½¿ç”¨ bf16
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
    # attn_implementation="flash_attention_2" # <--- åˆ é™¤æˆ–æ³¨é‡Šè¿™ä¸€è¡Œ
)
model.config.use_cache = False

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left" # DPO ç”Ÿæˆéœ€è¦å·¦å¡«å……

# --- 2. æ•°æ®å¤„ç† (Prompt/Chosen/Rejected) ---
print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path} ...")
if not os.path.exists(dataset_path):
    raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶: {dataset_path}ï¼Œè¯·å…ˆè¿è¡Œ process_dpo_data.py")

dataset = load_dataset("json", data_files=dataset_path, split="train")

def format_dpo_chat(row):
    """
    å°†æ•°æ®æ ¼å¼åŒ–ä¸º User/Assistant å¯¹è¯æ ¼å¼ã€‚
    """
    # Prompt: User æé—®
    prompt = tokenizer.apply_chat_template(
        [{"role": "user", "content": row["prompt"]}],
        tokenize=False,
        add_generation_prompt=True
    )
    
    # Chosen/Rejected: Assistant å›ç­” + EOS
    chosen = row["chosen"] + tokenizer.eos_token
    rejected = row["rejected"] + tokenizer.eos_token
    
    return {
        "prompt": prompt,
        "chosen": chosen,
        "rejected": rejected
    }

processed_dataset = dataset.map(format_dpo_chat, num_proc=8)

# --- 3. LoRA é…ç½® ---
model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)

# --- 4. DPO è®­ç»ƒå‚æ•° ---
training_args = DPOConfig(
    output_dir=output_dir,
    beta=0.1,                       # DPO æ¸©åº¦
    per_device_train_batch_size=8,  # æ˜¾å­˜å…è®¸çš„æƒ…å†µä¸‹å°½é‡å¤§
    gradient_accumulation_steps=4,
    learning_rate=5e-6,             # DPO å­¦ä¹ ç‡ (æ¯” SFT ä½)
    num_train_epochs=3,
    logging_steps=10,
    save_strategy="epoch",
    fp16=False,
    bf16=True,                      # A100 å¼€å¯ bf16
    optim="paged_adamw_32bit",
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
    remove_unused_columns=False,
    max_length=1024,
    max_prompt_length=512,
)

# --- 5. å¼€å§‹è®­ç»ƒ ---
print("åˆå§‹åŒ– DPOTrainer...")
trainer = DPOTrainer(
    model=model,
    ref_model=None, # LoRA æ¨¡å¼ä¸éœ€è¦åŠ è½½ ref_model
    args=training_args,
    train_dataset=processed_dataset,
    processing_class=tokenizer, # <--- ä¿®æ”¹è¿™é‡Œï¼šå°† tokenizer æ”¹ä¸º processing_class
    peft_config=peft_config,
)

print("ğŸš€ å¼€å§‹ DPO å¾®è°ƒ...")
trainer.train()
print("ğŸ‰ å¾®è°ƒå®Œæˆï¼")

# --- 6. ä¿å­˜ ---
print(f"æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {final_model_dir}")
trainer.save_model(final_model_dir)
tokenizer.save_pretrained(final_model_dir)
